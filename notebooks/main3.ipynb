{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from albumentations import Compose, RandomResizedCrop, HorizontalFlip, Normalize, RandomRotate90, ShiftScaleRotate, CoarseDropout\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters and configurations\n",
    "config = {\n",
    "    \"base_dir\": \"/Users/saahil/Desktop/Coding_Projects/DL/MicroscopicFungi/archive-2\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 4e-3,\n",
    "    \"height\": 224,\n",
    "    \"width\": 224,\n",
    "    \"channels\": 3,\n",
    "    \"num_folds\": 5,\n",
    "    \"patience\": 10,\n",
    "    \"seed\": 40,\n",
    "    \"log_dir\": \"./logs\",\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = config[\"log_dir\"]\n",
    "\n",
    "# Clear the log directory\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(config[\"log_dir\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FungiDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, subset='train'):\n",
    "        self.root_dir = os.path.join(root_dir, subset)\n",
    "        self.transform = transform\n",
    "        self.classes = ['H1', 'H2', 'H3', 'H5', 'H6']  # List of class names\n",
    "        self.image_paths, self.labels = self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        image_paths, labels = [], []\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            cls_dir = os.path.join(self.root_dir, cls)\n",
    "            if not os.path.exists(cls_dir):\n",
    "                raise FileNotFoundError(f\"Directory {cls_dir} does not exist.\")\n",
    "            for img_name in os.listdir(cls_dir):\n",
    "                img_path = os.path.join(cls_dir, img_name)\n",
    "                if os.path.isfile(img_path):\n",
    "                    image_paths.append(img_path)\n",
    "                    labels.append(label)\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(config[\"channels\"], 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * (config[\"height\"] // 16) * (config[\"width\"] // 16), 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(config[\"height\"], config[\"width\"], scale=(0.8, 1.0)),\n",
    "        HorizontalFlip(),\n",
    "        RandomRotate90(),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30),\n",
    "        CoarseDropout(max_holes=8, max_height=32, max_width=32),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, fold, epoch, best=False):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    filename = f'checkpoint_fold{fold}_epoch{epoch}{\"_best\" if best else \"\"}.pth'\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_checkpoint(model, optimizer, filename):\n",
    "#     checkpoint = torch.load(filename)\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#     return checkpoint['epoch']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model():\n",
    "#     dataset = FungiDataset(config[\"base_dir\"], transform=get_transforms(), subset='train')\n",
    "#     kf = KFold(n_splits=config[\"num_folds\"], shuffle=True, random_state=config[\"seed\"])\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "#         print(f\"Fold {fold}/{config['num_folds']}\")\n",
    "\n",
    "#         train_sampler = SubsetRandomSampler(train_idx)\n",
    "#         val_sampler = SubsetRandomSampler(val_idx)\n",
    "#         train_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "#         val_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=val_sampler)\n",
    "\n",
    "#         model = CustomCNN(num_classes=len(dataset.classes)).to(device)\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        \n",
    "        \n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "#         best_val_loss, patience_counter = float('inf'), 0\n",
    "#         best_model_path = f'checkpoint_fold{fold}_best.pth'\n",
    "\n",
    "#         for epoch in range(1, config[\"epochs\"] + 1):\n",
    "#             print(f\"Epoch {epoch}/{config['epochs']}\")\n",
    "\n",
    "#             train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#             val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "#             print(f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "#             writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "#             writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "#             writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "#             writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "#             writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "#             # Update the scheduler based on the validation loss\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 patience_counter = 0\n",
    "#                 print(f\"New best model found for fold {fold} at epoch {epoch}, saving model...\")\n",
    "#                 torch.save({\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'epoch': epoch,\n",
    "#                     'best_val_loss': best_val_loss,\n",
    "#                 }, best_model_path)\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "\n",
    "#             if patience_counter >= config[\"patience\"]:\n",
    "#                 print(\"Early stopping triggered\")\n",
    "#                 break\n",
    "\n",
    "#     writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# def train_model():\n",
    "#     dataset = FungiDataset(config[\"base_dir\"], transform=get_transforms(), subset='train')\n",
    "#     strat_kf = StratifiedKFold(n_splits=config[\"num_folds\"], shuffle=True, random_state=config[\"seed\"])\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(strat_kf.split(np.arange(len(dataset)), dataset.labels), 1):\n",
    "#         print(f\"Fold {fold}/{config['num_folds']}\")\n",
    "\n",
    "#         train_sampler = SubsetRandomSampler(train_idx)\n",
    "#         val_sampler = SubsetRandomSampler(val_idx)\n",
    "#         train_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "#         val_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=val_sampler)\n",
    "\n",
    "#         model = CustomCNN(num_classes=len(dataset.classes)).to(device)\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        \n",
    "        \n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "#         best_val_loss, patience_counter = float('inf'), 0\n",
    "#         best_model_path = f'checkpoint_fold{fold}_best.pth'\n",
    "\n",
    "#         for epoch in range(1, config[\"epochs\"] + 1):\n",
    "#             print(f\"Epoch {epoch}/{config['epochs']}\")\n",
    "\n",
    "#             train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#             val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "#             print(f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "#             writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "#             writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "#             writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "#             writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "#             writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "#             # Update the scheduler based on the validation loss\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 patience_counter = 0\n",
    "#                 print(f\"New best model found for fold {fold} at epoch {epoch}, saving model...\")\n",
    "#                 torch.save({\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'epoch': epoch,\n",
    "#                     'best_val_loss': best_val_loss,\n",
    "#                 }, best_model_path)\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "\n",
    "#             if patience_counter >= config[\"patience\"]:\n",
    "#                 print(\"Early stopping triggered\")\n",
    "#                 break\n",
    "\n",
    "#     writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def train_model():\n",
    "#     dataset = FungiDataset(config[\"base_dir\"], transform=get_transforms(), subset='train')\n",
    "#     class_weights = compute_class_weight('balanced', classes=np.arange(len(dataset.classes)), y=dataset.labels)\n",
    "#     class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "#     kf = KFold(n_splits=config[\"num_folds\"], shuffle=True, random_state=config[\"seed\"])\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "\n",
    "        \n",
    "#         print(f\"Fold {fold}/{config['num_folds']}\")\n",
    "#         train_labels = np.array(dataset.labels)[train_idx]\n",
    "#         val_labels = np.array(dataset.labels)[val_idx]\n",
    "#         print(f\"Fold {fold} - Train Class Distribution: {np.bincount(train_labels)}\")\n",
    "#         print(f\"Fold {fold} - Val Class Distribution: {np.bincount(val_labels)}\")\n",
    "\n",
    "\n",
    "\n",
    "#         train_sampler = SubsetRandomSampler(train_idx)\n",
    "#         val_sampler = SubsetRandomSampler(val_idx)\n",
    "#         train_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "#         val_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=val_sampler)\n",
    "\n",
    "#         model = CustomCNN(num_classes=len(dataset.classes)).to(device)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "#         best_val_loss, patience_counter = float('inf'), 0\n",
    "#         best_model_path = f'checkpoint_fold{fold}_best.pth'\n",
    "\n",
    "#         for epoch in range(1, config[\"epochs\"] + 1):\n",
    "#             print(f\"Epoch {epoch}/{config['epochs']}\")\n",
    "\n",
    "#             train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#             val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "#             print(f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "#             writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "#             writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "#             writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "#             writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "#             writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 patience_counter = 0\n",
    "#                 print(f\"New best model found for fold {fold} at epoch {epoch}, saving model...\")\n",
    "#                 torch.save({\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'epoch': epoch,\n",
    "#                     'best_val_loss': best_val_loss,\n",
    "#                 }, best_model_path)\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "\n",
    "#             if patience_counter >= config[\"patience\"]:\n",
    "#                 print(\"Early stopping triggered\")\n",
    "#                 break\n",
    "\n",
    "#     writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def train_model():\n",
    "    dataset = FungiDataset(config[\"base_dir\"], transform=get_transforms(), subset='train')\n",
    "    \n",
    "    # Compute class weights for handling class imbalance\n",
    "    class_weights = compute_class_weight('balanced', classes=np.arange(len(dataset.classes)), y=dataset.labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Use StratifiedKFold to ensure each fold has a similar class distribution\n",
    "    skf = StratifiedKFold(n_splits=config[\"num_folds\"], shuffle=True, random_state=config[\"seed\"])\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(len(dataset)), dataset.labels), 1):\n",
    "\n",
    "        print(f\"Fold {fold}/{config['num_folds']}\")\n",
    "        \n",
    "        # Extract the labels for the train and validation indices\n",
    "        train_labels = np.array(dataset.labels)[train_idx]\n",
    "        val_labels = np.array(dataset.labels)[val_idx]\n",
    "        print(f\"Fold {fold} - Train Class Distribution: {np.bincount(train_labels)}\")\n",
    "        print(f\"Fold {fold} - Val Class Distribution: {np.bincount(val_labels)}\")\n",
    "\n",
    "        # Set up the data samplers and loaders\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=val_sampler)\n",
    "\n",
    "        # Reinitialize the model for each fold\n",
    "        model = CustomCNN(num_classes=len(dataset.classes)).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "        best_val_loss, patience_counter = float('inf'), 0\n",
    "        best_model_path = f'checkpoint_fold{fold}_best.pth'\n",
    "\n",
    "        for epoch in range(1, config[\"epochs\"] + 1):\n",
    "            print(f\"Epoch {epoch}/{config['epochs']}\")\n",
    "\n",
    "            train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "            val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "            writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "            writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "            # Update the learning rate based on validation loss\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Save the model if it has the best validation loss so far\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                print(f\"New best model found for fold {fold} at epoch {epoch}, saving model...\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }, best_model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= config[\"patience\"]:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1 - Train Class Distribution: [800 800 800 800 800]\n",
      "Fold 1 - Val Class Distribution: [200 200 200 200 200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saahil/Desktop/Coding_Projects/DL/MicroscopicFungi/env1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4771, Acc: 35.17%, Val Loss: 0.2772, Val Acc: 40.80%\n",
      "New best model found for fold 1 at epoch 1, saving model...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0515, Acc: 41.00%, Val Loss: 0.2604, Val Acc: 43.60%\n",
      "New best model found for fold 1 at epoch 2, saving model...\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0184, Acc: 44.70%, Val Loss: 0.2501, Val Acc: 44.50%\n",
      "New best model found for fold 1 at epoch 3, saving model...\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0128, Acc: 45.90%, Val Loss: 0.2505, Val Acc: 44.50%\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9863, Acc: 49.35%, Val Loss: 0.2475, Val Acc: 47.80%\n",
      "New best model found for fold 1 at epoch 5, saving model...\n",
      "Fold 2/5\n",
      "Fold 2 - Train Class Distribution: [800 800 800 800 800]\n",
      "Fold 2 - Val Class Distribution: [200 200 200 200 200]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8472, Acc: 25.85%, Val Loss: 0.2688, Val Acc: 38.40%\n",
      "New best model found for fold 2 at epoch 1, saving model...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0738, Acc: 39.10%, Val Loss: 0.2565, Val Acc: 41.00%\n",
      "New best model found for fold 2 at epoch 2, saving model...\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0141, Acc: 44.73%, Val Loss: 0.2447, Val Acc: 46.60%\n",
      "New best model found for fold 2 at epoch 3, saving model...\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9933, Acc: 46.95%, Val Loss: 0.2371, Val Acc: 50.20%\n",
      "New best model found for fold 2 at epoch 4, saving model...\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9812, Acc: 47.83%, Val Loss: 0.2419, Val Acc: 46.80%\n",
      "Fold 3/5\n",
      "Fold 3 - Train Class Distribution: [800 800 800 800 800]\n",
      "Fold 3 - Val Class Distribution: [200 200 200 200 200]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2808, Acc: 19.75%, Val Loss: 0.3219, Val Acc: 20.00%\n",
      "New best model found for fold 3 at epoch 1, saving model...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1708, Acc: 33.52%, Val Loss: 0.2635, Val Acc: 47.50%\n",
      "New best model found for fold 3 at epoch 2, saving model...\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0352, Acc: 44.35%, Val Loss: 0.2512, Val Acc: 47.80%\n",
      "New best model found for fold 3 at epoch 3, saving model...\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0074, Acc: 47.73%, Val Loss: 0.2494, Val Acc: 48.00%\n",
      "New best model found for fold 3 at epoch 4, saving model...\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9931, Acc: 47.75%, Val Loss: 0.2474, Val Acc: 48.10%\n",
      "New best model found for fold 3 at epoch 5, saving model...\n",
      "Fold 4/5\n",
      "Fold 4 - Train Class Distribution: [800 800 800 800 800]\n",
      "Fold 4 - Val Class Distribution: [200 200 200 200 200]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1223, Acc: 27.15%, Val Loss: 0.2653, Val Acc: 39.90%\n",
      "New best model found for fold 4 at epoch 1, saving model...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0483, Acc: 41.33%, Val Loss: 0.2529, Val Acc: 48.00%\n",
      "New best model found for fold 4 at epoch 2, saving model...\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0530, Acc: 41.60%, Val Loss: 0.2542, Val Acc: 48.50%\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1128, Acc: 40.62%, Val Loss: 0.2589, Val Acc: 40.10%\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0569, Acc: 42.65%, Val Loss: 0.2569, Val Acc: 43.50%\n",
      "Fold 5/5\n",
      "Fold 5 - Train Class Distribution: [800 800 800 800 800]\n",
      "Fold 5 - Val Class Distribution: [200 200 200 200 200]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1504, Acc: 27.88%, Val Loss: 0.2725, Val Acc: 38.90%\n",
      "New best model found for fold 5 at epoch 1, saving model...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0676, Acc: 41.67%, Val Loss: 0.2679, Val Acc: 42.30%\n",
      "New best model found for fold 5 at epoch 2, saving model...\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0217, Acc: 43.90%, Val Loss: 0.2556, Val Acc: 46.80%\n",
      "New best model found for fold 5 at epoch 3, saving model...\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0991, Acc: 44.23%, Val Loss: 0.2554, Val Acc: 46.70%\n",
      "New best model found for fold 5 at epoch 4, saving model...\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0188, Acc: 44.30%, Val Loss: 0.2647, Val Acc: 37.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
